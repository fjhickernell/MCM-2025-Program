\chapter{Sunday Tutorials}


\begin{talk}
 {Quasi-Monte Carlo Methods:  What, Why, and How?}% [1] talk title
 {Fred J.~Hickernell}% [2] speaker name
 {Department of Applied Mathematics and Center for Interdisciplinary Scientific Computation, Illinois Institute of Technology}% [3] affiliations
 {hickernell@iit.edu}% [4] email
 {}% [5] coauthors
 {}% [6] special session
 {\timeslot{Sunday, Aug 18, 2024}{14:15}{15:45}{STC 1012}}% [7] time slot
 {TUT02}% [8] talk id
 {photo}% [9] session id or photo
Many problems in  quantitative finance, uncertainty quantification, and other areas can be formulated as computing $\mu := \mathbb{E}(Y)$, where instances of $Y:=f(\boldsymbol{X})$ are generated by numerical simulation. The population mean, $\mu$, can be approximated by the sample mean, $\hat{\mu}_n := n^{-1} \sum_{i=1}^n f(\boldsymbol{X}_i)$.  Computing $\mu$ is equivalent to computing a $d$-dimensional integral.

Quasi-Monte Carlo methods replace independent and identically distributed  sequences of random vectors, $\{\boldsymbol{X}_1, \boldsymbol{X}_2, \ldots \}$, by low discrepancy sequences.  This accelerates the convergence of $\hat{\mu}_n$ to $\mu$ as $n \to \infty$. 


This tutorial describes  low discrepancy sequences  and their quality measures.  We demonstrate the performance gains possible with quasi-Monte Carlo methods.  Moreover, we describe how to formulate problems to realize the most increase in performance using quasi-Monte Carlo methods.  We also briefly describe the use of quasi-Monte Carlo methods for problems beyond computing the mean.

\end{talk}

\clearpage


\begin{talk}
 {Grey-box Bayesian Optimization}% [1] talk title
 {Peter Frazier}% [2] speaker name
 {Cornell University}% [3] affiliations
 {pf98@cornell.edu}% [4] email
 {}% [5] coauthors
 {}% [6] special session
 {\timeslot{Sunday, Aug 18, 2024}{16:00}{17:30}{STC 1012}}% [7] time slot
 {TUT01}% [8] talk id
 {photo}% [9] session id or photo
Bayesian optimization (BayesOpt) is powerful tool for optimizing objective functions evaluated using Monte Carlo or Quasi Monte Carlo. It aims to produce an approximate global optimum with few objective function evaluations by combining a machine-learning-based surrogate for the objective function (often a Gaussian process) with a decision-theoretic acquisition function that efficiently directs sampling effort.  It is widely used for optimizing social media platforms, tuning hyperparameters in deep neural networks, designing new drugs and materials, and beyond. While BayesOpt has historically been deployed as a black-box optimizer, recent advances show orders-of-magnitude improvement through new grey-box methods that "peek inside the box".  For example, consider optimizing product interventions in simulation to improve a ridesharing market averaged over exogenous shocks. New grey-box BayesOpt methods use a machine learning surrogate for how market outcomes depend on both the exogenous shocks and the product intervention to evaluate for the most informative shocks first rather than sampling blindly. This tutorial introduces Gaussian process regression, standard (black-box) BayesOpt, and new grey-box methods, motivating with examples from machine learning, engineering and the physical sciences. We close by identifying research opportunities where the MCQMC community can have a particularly large impact.

\end{talk}

\clearpage