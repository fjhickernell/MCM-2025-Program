\documentclass[12pt,a4paper,figuresright]{book}

\usepackage{amsmath,amssymb}
\usepackage{tabularx,graphicx,url,xcolor,rotating,multicol,epsfig,colortbl}

\setlength{\textheight}{25.2cm}
\setlength{\textwidth}{16.5cm} %\setlength{\textwidth}{18.2cm}
\setlength{\voffset}{-1.6cm}
\setlength{\hoffset}{-0.3cm} %\setlength{\hoffset}{-1.2cm}
\setlength{\evensidemargin}{-0.3cm} 
\setlength{\oddsidemargin}{0.3cm}
\setlength{\parindent}{0cm} 
\setlength{\parskip}{0.3cm}

% -- adding a talk
\newenvironment{talk}[6]% [1] talk title
                         % [2] speaker name, [3] affiliations, [4] email,
                         % [5] coauthors, [6] special session
                         % [7] time slot
                         % [8] talk id, [9] session id or photo
 {%\needspace{6\baselineskip}%
  \vskip 0pt\nopagebreak%
%   \colorbox{gray!20!white}{\makebox[0.99\textwidth][r]{}}\nopagebreak%
%   \ifthenelse{\equal{#9}{photo}}{%
%                     \\\\\colorbox{gray!20!white}{\makebox{\includegraphics[width=3cm]{#8}}}\nopagebreak}{}%
 \vskip 0pt\nopagebreak%
%  \label{#8}%
  \textbf{#1}\vspace{3mm}\\\nopagebreak%
  \textit{#2}\\\nopagebreak%
  #3\\\nopagebreak%
  \url{#4}\vspace{3mm}\\\nopagebreak%
  \ifthenelse{\equal{#5}{}}{}{Coauthor(s): #5\vspace{3mm}\\\nopagebreak}%
  \ifthenelse{\equal{#6}{}}{}{Special session: #6\quad \vspace{3mm}\\\nopagebreak}%
 }
 {\vspace{1cm}\nopagebreak}%

\pagestyle{empty}

% ------------------------------------------------------------------------
% Document begins here
% ------------------------------------------------------------------------
\begin{document}
	
\begin{talk}
  {Convergence of $\Phi$-Divergence and $\Phi$-Mutual Information Along Langevin Markov Chains}% [1] talk title
  {Siddharth Mitra}% [2] speaker name
  {Yale University}% [3] affiliations
  {siddharth.mitra@yale.edu}% [4] email
  {Jiaming Liang, Andre Wibisono}% [5] coauthors
  {}% [6] special session. Leave this field empty for contributed talks. 
				% Insert the title of the special session if you were invited to give a talk in a special session.
			
The mixing time of a Markov chain determines when the marginal law of the Markov chain is close to the stationary distribution and can be studied in many statistical divergences such as KL divergence and chi-squared divergence, all the way to families of divergences such as $\Phi$-divergences. However, the mixing time does not determine the dependency between samples along the Markov chain, which can be measured in terms of their mutual information, chi-squared mutual information, or more generally their $\Phi$-mutual information. In this talk we study the mixing time of Langevin Markov chains in $\Phi$-divergence and also study the $\Phi$-mutual information between the iterates. The Markov chains we focus on are the Langevin Dynamics in continuous-time, and the Unadjusted Langevin Algorithm and Proximal Sampler in discrete-time and we show that for these Markov chains, the $\Phi$-divergence and the $\Phi$-mutual information decreases exponentially fast. Our proof technique is based on showing the Strong Data Processing Inequalities (SDPIs) hold along the Markov chains. To prove fast mixing of the Markov chains, we show the SDPIs hold for the stationary distribution. In contrast, to prove the contraction of $\Phi$-mutual information, we need to show the SDPIs hold along the entire trajectories of the Markov chains; we prove this when the iterates along the Markov chains satisfy the corresponding $\Phi$-Sobolev inequality.


\medskip


\end{talk}

\end{document}

