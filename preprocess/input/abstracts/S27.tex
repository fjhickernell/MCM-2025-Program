\documentclass[12pt,a4paper,figuresright]{book}

\usepackage{amsmath,amssymb}
\usepackage{tabularx,multirow,graphicx,url,wrapfig,xcolor,rotating,multicol,epsfig,colortbl}

\setlength{\textheight}{25.2cm}
\setlength{\textwidth}{16.5cm} %\setlength{\textwidth}{18.2cm}
\setlength{\voffset}{-1.6cm}
\setlength{\hoffset}{-0.3cm} %\setlength{\hoffset}{-1.2cm}
\setlength{\evensidemargin}{-0.3cm}
\setlength{\oddsidemargin}{0.3cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.3cm}

\renewcommand{\topfraction}{1}
\renewcommand{\textfraction}{0}
\setlength{\floatsep}{12pt plus 2pt minus 2pt}

\newcommand{\organizer}[3]{%
	{\textit{#1}}\\\nopagebreak%
	#2\\\nopagebreak%
	\url{#3}\vspace{3mm}\\\nopagebreak%
	}

\newenvironment{session}[5] % [1] session title
							% [2] number of organizers
                            % [3] organizer 1 info
                            % [4] organizer 2 info
                            % [5] organizer 3 info
                            % [6] session id for later
 {%\needspace{6\baselineskip}
  \vskip 0pt\nopagebreak%
  %\label{#5}%
  \textbf{#1}\vspace{3mm}\\\nopagebreak%
  \ifthenelse{\equal{#2}{1}}{Organizer:}{Organizers:}%
  \vspace{2mm}\\\nopagebreak%
  #3
  \ifthenelse{\equal{#2}{2}}{#4}{}%
  \ifthenelse{\equal{#2}{3}}{#4#5}{}%
  \quad\\\nopagebreak%
  %Session Description:\vspace{3mm}\\\nopagebreak%
 }
 {\nopagebreak}%

\pagestyle{empty}

% ------------------------------------------------------------------------
% Document begins here
% ------------------------------------------------------------------------
\begin{document}

%Input the relevant information below
\begin{session}
  {Recent Advances in Stochastic Gradient Descent}% [1] session title
  {1} %[2]  number of organizers
  {\organizer{Jing Dong}% organizer one name
    {Columbia University}% organizer one affiliations
    {jing.dong@gsb.columbia.edu}}% organizer one email
  {\organizer{}% organizer two name, if needed
	{}% organizer two affiliations, if needed
	{}}% organizer two email
  {\organizer{}% organizer three name
	{}% organizer three affiliations
	{}}% organizer three email
  

Stochastic Gradient Descent (SGD) is a cornerstone optimization method in machine learning,
renowned for its efficiency in handling large-scale data. Its iterative approach enables
the processing of extensive datasets by updating model parameters using randomly selected
data subsets, thereby reducing computational costs. Despite its widespread adoption, traditional
SGD faces challenges such as convergence to sharp minima, and sensitivity to data
distribution shifts. Addressing these challenges is crucial for enhancing model generalization,
robustness, and overall performance in diverse applications. This session aims to delve into
recent developments that address these challenges in SGD, presenting innovative methodologies
and theoretical insights to enhance its effectiveness in complex learning scenarios.

The session will have three to four speakers. Currently, the confirmed speakers are Jose
Blanchet (Stanford University), Chang-Han Rhee (Northwestern University), and Jing Dong
(Columbia University). Each will present their recent works on stochastic gradient descent,
ranging from SGD and heavy-tailed phenomenon to SGD with adaptively generated data.

Collectively, these talks will shed light on cutting-edge advancements in SGD methodologies,
providing both theoretical frameworks and practical strategies to enhance optimization in
complex, real-world applications.

\end{session}

\end{document}

