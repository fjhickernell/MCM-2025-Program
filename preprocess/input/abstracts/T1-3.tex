\documentclass[12pt,a4paper,figuresright]{book}

\usepackage{amsmath,amssymb}
\usepackage{tabularx,graphicx,url,xcolor,rotating,multicol,epsfig,colortbl}

\setlength{\textheight}{25.2cm}
\setlength{\textwidth}{16.5cm} %\setlength{\textwidth}{18.2cm}
\setlength{\voffset}{-1.6cm}
\setlength{\hoffset}{-0.3cm} %\setlength{\hoffset}{-1.2cm}
\setlength{\evensidemargin}{-0.3cm}
\setlength{\oddsidemargin}{0.3cm}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.3cm}

% -- adding a talk
\newenvironment{talk}[6]% [1] talk title
                         % [2] speaker name, [3] affiliations, [4] email,
                         % [5] coauthors, [6] special session
                         % [7] time slot
                         % [8] talk id, [9] session id or photo
 {%\needspace{6\baselineskip}%
  \vskip 0pt\nopagebreak%
%   \colorbox{gray!20!white}{\makebox[0.99\textwidth][r]{}}\nopagebreak%
%   \ifthenelse{\equal{#9}{photo}}{%
%                     \\\\\colorbox{gray!20!white}{\makebox{\includegraphics[width=3cm]{#8}}}\nopagebreak}{}%
 \vskip 0pt\nopagebreak%
%  \label{#8}%
  \textbf{#1}\vspace{3mm}\\\nopagebreak%
  \textit{#2}\\\nopagebreak%
  #3\\\nopagebreak%
  \url{#4}\vspace{3mm}\\\nopagebreak%
  \ifthenelse{\equal{#5}{}}{}{Coauthor(s): #5\vspace{3mm}\\\nopagebreak}%
  \ifthenelse{\equal{#6}{}}{}{Special session: #6\quad \vspace{3mm}\\\nopagebreak}%
 }
 {\vspace{1cm}\nopagebreak}%

\pagestyle{empty}

% ------------------------------------------------------------------------
% Document begins here
% ------------------------------------------------------------------------
\begin{document}
	
\begin{talk}
  {Theoretical guarantees for lifted samplers}% [1] talk title
  {Philippe Gagnon}% [2] speaker name
  {Université de Montréal}% [3] affiliations
  {philippe.gagnon.3@umontreal.ca}% [4] email
  {Florian Maire}% [5] coauthors
  {}% [6] special session. Leave this field empty for contributed talks.
				% Insert the title of the special session if you were invited to give a talk in a special session.
			
The work I would like to present is about a particular class of Markov chain Monte Carlo (MCMC) methods which use non-reversible Markov chains commonly referred to as \textit{lifted Markov chains}; the methods are commonly referred to as \textit{lifted samplers}. The methods are not particularly new (they date back at least to Horowitz [1]), but they have recently been the subject of significant research work motivated by a general belief that, in statistical applications, they lead to more efficient estimators than their reversible counterparts which correspond to Metropolis--Hastings (MH) algorithms (see, e.g., Andrieu and Livingstone [2]). It was thus somewhat surprising to observe that it is not always the case in some recent work (see, e.g., Gagnon and Maire [3]). One can thus wonder what degree of inefficiency these chains may exhibit in worst-case scenarios. This is an important question given that lifted samplers are popular in practice, a consequence of the fact that they are often as easy to implement on a computer as their MH counterparts and often have the same computational complexity.

The main contribution of our work is to provide an answer to this question under arguably the most general framework. We proceed by leveraging the seminal work of Tierney [4] to define a lifted version of a generalized MH algorithm. Virtually any (reversible) MCMC method can be seen as a special case of this generalized MH algorithm, ranging from the traditional MH algorithm of Hastings [5] to the reversible jump algorithm of Green [6]. Our main theoretical result allows for a comparison between the generalized MH algorithm and its lifted version in terms of the variance of produced estimators. It essentially guarantees that the variance of estimators produced by the lifted version cannot be more than twice that of estimators produced by the generalized MH algorithm. This result indicates that, while there is potentially a lot to gain from lifting a Markov chain, there is not much to lose. We also show that our result is optimal, in the sense that it is not possible to improve on the factor 2 without additional assumptions. The definition of the lifted version of the generalized MH algorithm allows to understand how a lifted sampler can be constructed under such a general framework, which adds a methodological contribution to our theoretical contribution.

The efficiency of Markov chains is traditionally assessed by studying the characteristics of their Markov transition operators. To establish our theoretical result, we needed to connect the efficiency of two significantly different operators (those of the MH and lifted algorithms): in addition to not be defined on the same domain, one is self-adjoint while the other is not, which further complicates the analysis. One of our main achievements was to identify a specific auxiliary operator which acts a bridge and allows to connect the efficiency of the two aforementioned operators. This auxiliary operator is compared to the MH one through a Peskun ordering [7] established via a careful analysis of the Markov kernels, yielding sharp bounds. The connections between the MH and lifted algorithms is completed by comparing the auxiliary operator with the lifted one using a result in Andrieu and Livingstone [2].

\medskip

\begin{enumerate}
	\item[{[1]}] Horowitz, A.\ M.\ (1991) A generalized guided Monte Carlo algorithm. Phys.\ Lett.\ B, \textbf{268}, 247--252.
	\item[{[2]}] Andrieu, C.\ and Livingstone, S.\ (2021) Peskun--Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario. Ann.\ Statist., \textbf{49}, 1958--1981.
    \item[{[3]}] Gagnon, P.\ and Maire, F.\ (2024) An asymptotic Peskun ordering and its application to lifted samplers. Bernoulli, \textbf{30}, 2301--2325.
    \item[{[4]}] Tierney, L.\ (1998) A note on Metropolis--Hastings kernels for general state spaces. Ann.\ Appl.\ Probab., \textbf{8}, 1--9.
    \item[{[5]}] Hastings, W.\ K.\ (1970) Monte Carlo sampling methods using Markov chains and their applications. Biometrika, \textbf{57}, 97--109.
    \item[{[6]}] Green, P.\ J.\ (1995) Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, \textbf{82}, 711--732.
    \item[{[7]}] Peskun, P.\ (1973) Optimum Monte-Carlo sampling using Markov chains. Biometrika, \textbf{60}, 607--612
\end{enumerate}


\end{talk}

\end{document}

