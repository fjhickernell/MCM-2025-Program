\documentclass[12pt,a4paper,figuresright]{book}

\usepackage{amsmath,amssymb}
\usepackage{tabularx,graphicx,url,xcolor,rotating,multicol,epsfig,colortbl}

\setlength{\textheight}{25.2cm}
\setlength{\textwidth}{16.5cm} %\setlength{\textwidth}{18.2cm}
\setlength{\voffset}{-1.6cm}
\setlength{\hoffset}{-0.3cm} %\setlength{\hoffset}{-1.2cm}
\setlength{\evensidemargin}{-0.3cm} 
\setlength{\oddsidemargin}{0.3cm}
\setlength{\parindent}{0cm} 
\setlength{\parskip}{0.3cm}

% -- adding a talk
\newenvironment{talk}[6]% [1] talk title
                         % [2] speaker name, [3] affiliations, [4] email,
                         % [5] coauthors, [6] special session
                         % [7] time slot
                         % [8] talk id, [9] session id or photo
 {%\needspace{6\baselineskip}%
  \vskip 0pt\nopagebreak%
%   \colorbox{gray!20!white}{\makebox[0.99\textwidth][r]{}}\nopagebreak%
%   \ifthenelse{\equal{#9}{photo}}{%
%                     \\\\\colorbox{gray!20!white}{\makebox{\includegraphics[width=3cm]{#8}}}\nopagebreak}{}%
 \vskip 0pt\nopagebreak%
%  \label{#8}%
  \textbf{#1}\vspace{3mm}\\\nopagebreak%
  \textit{#2}\\\nopagebreak%
  #3\\\nopagebreak%
  \url{#4}\vspace{3mm}\\\nopagebreak%
  \ifthenelse{\equal{#5}{}}{}{Coauthor(s): #5\vspace{3mm}\\\nopagebreak}%
  \ifthenelse{\equal{#6}{}}{}{Special session: #6\quad \vspace{3mm}\\\nopagebreak}%
 }
 {\vspace{1cm}\nopagebreak}%

\pagestyle{empty}

% ------------------------------------------------------------------------
% Document begins here
% ------------------------------------------------------------------------
\begin{document}
	
\begin{talk}
  {Fast Gaussian Processes}% [1] talk title
  {Aleksei G Sorokin}% [2] speaker name
  {Illinois Institute of Technology, Department of Applied Mathematics. \\ Sandia National Laboratories.}% [3] affiliations
  {asorokin@hawk.iit.edu}% [4] email
  {}%Pieterjan M Robbe, Fred J Hickernell}% [5] coauthors
  {Hardware and Software for (Quasi-)Monte Carlo Algorithms, Part 2}% [6] special session. Leave this field empty for contributed talks. 
				% Insert the title of the special session if you were invited to give a talk in a special session.

        Gaussian process regression (GPR) on $N$ data points typically costs $\mathcal{O}(N^3)$ as one must compute the inverse and determinant of a dense, unstructured Gram matrix. Here we present \texttt{fastgps}\footnote{\url{https://alegresor.github.io/fastgps/}}, a Python software which performs GPR at only $\mathcal{O}(N \log N)$ cost by forcing nice structure into the Gram matrices. Specifically, when one controls the design of experiments and is willing to use special kernel forms, pairing certain low discrepancy (LD) sequences with shift invariant kernels yields Gram matrices diagonalizable by fast transforms. Two such classes are:
        \begin{enumerate}
          \item Pairing \emph{lattice points} with \emph{shift invariant (SI) kernels} gives circulant Gram matrices diagonalizable by the fast Fourier transform (FFT). 
          \item Pairing \emph{digital nets} with \emph{digitally shift invariant (DSI) kernels} gives Gram matrices diagonalizable by the fast Walsh Hadamard transform (FWHT). 
        \end{enumerate}
        \texttt{fastgps} supports a number of features which will be discussed. 
        \begin{enumerate}
          \item \textbf{Kernel hyperparameter optimization} of marginal log likelihood (MLL), cross validation (CV), or generalized cross validation (GCV) loss.
          \item \textbf{Fast Bayesian cubature} for uncertainty quantification in Quasi-Monte Carlo. 
          \item \textbf{Fast multi-task GPR} with support for different sample sizes for each task. This is useful for multi-fidelity simulations and Multilevel Monte Carlo (MLMC). 
          \item \textbf{Efficient variance projections} for non-greedy Bayesian optimization in MLMC.
          \item \textbf{Derivative informed GPR} for simulations coupled with automatic differentiation. 
          \item \textbf{Batched GPR} for simultaneously modeling vector-output simulations.
          \item \textbf{GPU support} enabled by the \texttt{PyTorch}\footnote{\url{https://pytorch.org/}} stack . 
          \item \textbf{Flexible LD sequences and SI/DSI kernels} from \texttt{QMCPy}\footnote{\url{https://qmcsoftware.github.io/QMCSoftware/}}.
        \end{enumerate}



        %Many scientific problems require modeling the output of an expensive simulation for which low fidelity surrogates exist. One would like to exploit cheap evaluations of these low fidelity models in order to maximize information of the true (maximum fidelity) model with minimum cost. Multi-index Gaussian processes (MIGPs) provide a natural framework to model such multi-fidelity simulations. We show that, provided one controls the design of experiments, MIGP regression can be performed quickly by pairing certain low discrepancy sequences with special GP kernels. In the simplified case of MIGP with $L$ fidelities and $N$ samples at each fidelity, the $\mathcal{O}(N^3L^3)$ cost of standard MIGP can be reduced to $\mathcal{O}(N \log N L^2 + N L^3)$ using the proposed fast MIGP methods. We apply fast MIGP methods in the context of multilevel Monte Carlo (MLMC) where we target the expectation of the maximum fidelity model. The resulting method employs only one randomization of a low discrepancy point set, and the proposed acquisition function permits look ahead schemes for non-greedy algorithms. This makes it an ideal drop-in replacement for standard MLMC methods based on IID points. Our Python software for this project is based on the \texttt{QMCPy} package \url{https://github.com/QMCSoftware/QMCSoftware}, our implementation of fast MIGPs at \url{https://github.com/alegresor/FastGaussianProcesses}, and the ML(Q)MC library \url{https://github.com/PieterjanRobbe/mlqmcpy}. 




        %This talk will highlight recent improvements to the QMCPy Python package: a unified library for Quasi-Monte Carlo methods and computations related to low discrepancy sequences. We will describe routines for low discrepancy point set generation, randomization, and application to fast kernel methods. Specifically, we will discuss generators for lattices, digital nets, and Halton point sets with randomizations including random permutations / shifts, linear matrix scrambling, and nested uniform scrambling. Routines for working with higher-order digital nets and scramblings will also be detailed. For kernel methods, we provide implementations of special shift-invariant and digitally-shift invariant kernels along with fast Gram matrix operations facilitated by the bit-reversed Fast Fourier Transform (FFT), the bit-reversed inverse FFT (IFFT), and the Fast Walsh Hadamard Transform (FWHT). We will also describe methods to quickly update the matrix-vector product or linear system solution after doubling the number of points in a lattice or digital net in natural order. Generalizations to fast Gaussian process regression with derivative information will be discussed if time permits. 
%\medskip

% \begin{enumerate}
% 	\item[{[1]}] Sorokin, A. (2025). A Unified Implementation of Quasi-Monte Carlo Generators, Randomization Routines, and Fast Kernel Methods. arXiv preprint arXiv:2502.14256.
% \end{enumerate}

\end{talk}

\end{document}