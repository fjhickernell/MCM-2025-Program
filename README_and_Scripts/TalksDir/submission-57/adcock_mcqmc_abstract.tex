\documentclass[12pt,a4paper,figuresright]{book}

\usepackage{amsmath,amssymb}
\usepackage{tabularx,graphicx,url,xcolor,rotating,multicol,epsfig,colortbl}

\setlength{\textheight}{25.2cm}
\setlength{\textwidth}{16.5cm} %\setlength{\textwidth}{18.2cm}
\setlength{\voffset}{-1.6cm}
\setlength{\hoffset}{-0.3cm} %\setlength{\hoffset}{-1.2cm}
\setlength{\evensidemargin}{-0.3cm} 
\setlength{\oddsidemargin}{0.3cm}
\setlength{\parindent}{0cm} 
\setlength{\parskip}{0.3cm}

\usepackage{bm}

% -- adding a talk
\newenvironment{talk}[6]% [1] talk title
                         % [2] speaker name, [3] affiliations, [4] email,
                         % [5] coauthors, [6] special session
                         % [7] time slot
                         % [8] talk id, [9] session id or photo
 {%\needspace{6\baselineskip}%
  \vskip 0pt\nopagebreak%
%   \colorbox{gray!20!white}{\makebox[0.99\textwidth][r]{}}\nopagebreak%
%   \ifthenelse{\equal{#9}{photo}}{%
%                     \\\\\colorbox{gray!20!white}{\makebox{\includegraphics[width=3cm]{#8}}}\nopagebreak}{}%
 \vskip 0pt\nopagebreak%
%  \label{#8}%
  \textbf{#1}\vspace{3mm}\\\nopagebreak%
  \textit{#2}\\\nopagebreak%
  #3\\\nopagebreak%
  \url{#4}\vspace{3mm}\\\nopagebreak%
  \ifthenelse{\equal{#5}{}}{}{Coauthor(s): #5\vspace{3mm}\\\nopagebreak}%
  \ifthenelse{\equal{#6}{}}{}{Special session: #6\quad \vspace{3mm}\\\nopagebreak}%
 }
 {\vspace{1cm}\nopagebreak}%

\pagestyle{empty}

% ------------------------------------------------------------------------
% Document begins here
% ------------------------------------------------------------------------
\begin{document}
	
\begin{talk}
  {Optimal approximation of infinite-dimensional, Banach-valued, holomorphic functions from i.i.d. samples}% [1] talk title
  {Ben Adcock}% [2] speaker name
  {Simon Fraser University}% [3] affiliations
  {ben_adcock@sfu.ca}% [4] email
  {Simone Brugiapaglia, Nick Dexter, Sebastian Moraga}% [5] coauthors
  {Function recovery and discretization problems}% [6] special session. Leave this field empty for contributed talks. 
				% Insert the title of the special session if you were invited to give a talk in a special session.
		
		
		
	
Approximating Banach-valued functions of infinitely many variables from samples has gained increasing attention in computational science and engineering over the last decade, especially in computational uncertainty quantification, where such functions notably arise as solution maps to parametric or stochastic differential equations. 
Such problems are often highly data scarce. Therefore, it important to construct accurate approximations from limited samples.

In this talk, we consider the class of infinite-dimensional, Banach-valued, $(\bm{b},\varepsilon)$-holomorphic functions. This class was first introduced in the context of parametric differential equations to model the regularity of solution maps to many such problems. Previous studies have developed polynomial approximation theory for this class, demonstrating algebraic rates of convergence of best $n$-term polynomial approximations of the form $n^{1/2-1/p}$ whenever $\bm{b} \in \ell^p(\mathbb{N})$ for some $0 < p < 1$. However, these results say nothing about constructing approximations from finite samples. In this talk, we focus on this latter problem.

First, we establish lower bounds. We show that the \textit{(adaptive) $m$-widths} can decay at best like $c \cdot m^{1/2-1/p}$. Hence no combination of (adaptive) linear samples and (linear or nonlinear) recovery procedure can attain faster rates. Moreover, this can only be obtained when $\bm{b}$ belongs to the monotone $\ell^p$-space $\ell^p_{\mathsf{M}}(\mathbb{N})$, despite the aforementioned best approximation rates.
Second, we establish upper bounds that match these rates up to polylogarithmic factors. We consider Monte Carlo sampling -- i.e., i.i.d.\ pointwise samples from the underlying probability measure -- this being highly relevant to applications. We then use ideas from compressed sensing to devise a polynomial approximation scheme based on a certain weighted $\ell^1$-minimization procedure. This procedure requires no knowledge of the parameter $\bm{b}$, making it suitable for the practical \textit{unknown anisotropy} problem. A consequence of these results is that i.i.d.\ pointwise samples constitute near-optimal information for this problem.

Finally, we conclude by drawing connections between this work and the topic of operator learning with deep neural networks. In particular, we show how this work leads to novel \textit{practical existence theorems} for deep learning of certain infinite-dimensional operators.


\medskip

%If you would like to include references, please do so by creating a simple list numbered by [1], [2], [3], \ldots. See example below.
%Please do not use the \texttt{bibliography} environment or \texttt{bibtex} files.
%APA reference style is recommended.
\begin{enumerate}
\item[{[1]}] Adcock, Ben, Brugiapaglia, Simone, Dexter, Nick \& Moraga, Sebastian (2024). 
{\it Near-optimal learning of Banach-valued, high-dimensional functions via deep neural networks.} arXiv:2211.12633.
	\item[{[2]}] Adcock, Ben, Dexter, Nick \& Moraga, Sebastian (2024). {\it Optimal approximation of infinite-dimensional holomorphic functions}. Calcolo, 61:12.
	\item[{[3]}] Adcock, Ben, Dexter, Nick \& Moraga, Sebastian (2024). {\it Optimal approximation of infinite-dimensional holomorphic functions II: recovery from i.i.d. pointwise samples}. arXiv:2310.16940.
\end{enumerate}

%Equations may be used if they are referenced. Please note that the equation numbers may be different (but will be cross-referenced correctly) in the final program book.
\end{talk}

\end{document}

